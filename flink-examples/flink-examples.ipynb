{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flink Execution Environment\n",
    "\n",
    "To connect to Flink cluster you need something called an _execution environment_.\n",
    "\n",
    "Since we use Python, we will rely on Table API.\n",
    "\n",
    "During this lab session we will only use Flink's batch processing engine, although Flink is mostly famous as a streaming processor.\n",
    "The reason is that streaming applications for Flink are written usually in Java, not Python.\n",
    "\n",
    "Nevertheless, Table API is _the same_ for working with streams and batches, so let's get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyflink.dataset import ExecutionEnvironment\n",
    "from pyflink.table import TableConfig, BatchTableEnvironment\n",
    "\n",
    "exec_env = ExecutionEnvironment.get_execution_environment()\n",
    "exec_env.set_parallelism(1)\n",
    "t_config = TableConfig()\n",
    "t_env = BatchTableEnvironment.create(exec_env, t_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and Writing Data\n",
    "\n",
    "Again, because of the limitedness of the Python API, we will work with JSON files as with CSVs with one column.\n",
    "\n",
    "Since we use Table API, we first should register our files as tables in Flink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyflink.table.descriptors.BatchTableDescriptor at 0x7fef14fb9490>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyflink.table.descriptors import Schema, OldCsv, FileSystem\n",
    "from pyflink.table import DataTypes\n",
    "\n",
    "infile = \"/workdir/boris/data/yelp_dataset/yelp_academic_dataset_review.json\"\n",
    "(\n",
    "    t_env.connect(FileSystem().path(infile))\n",
    "    .with_format(OldCsv())\n",
    "    .with_schema(Schema().field(\"line\", DataTypes.STRING()))\n",
    "    .create_temporary_table(\"input_table\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyflink.table.descriptors.BatchTableDescriptor at 0x7fef14745ed0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    t_env.connect(FileSystem().path(\"result.json\"))\n",
    "    .with_format(OldCsv())\n",
    "    .with_schema(Schema().field(\"line\", DataTypes.STRING()))\n",
    "    .create_temporary_table(\"output_table\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding a Flink Job\n",
    "\n",
    "Typical Flink job should have a source, a sink, and some tasks to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    t_env.from_path(\"input_table\")\n",
    "    .select(\"\"\"\n",
    "    CONCAT('{\"total_lines\": ', count(1), '}') AS line\n",
    "    \"\"\")\n",
    "    .insert_into(\"output_table\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: impossible de supprimer « result.json »: Aucun fichier ou dossier de ce type\n"
     ]
    }
   ],
   "source": [
    "# Before running the job we remove the previous results if any\n",
    "!rm result.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30366\n"
     ]
    }
   ],
   "source": [
    "# After running the job we can observe some statistics of it's execution\n",
    "\n",
    "res = t_env.execute(\"io_job\")\n",
    "print(res.get_net_runtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"total_lines\": 8021122}\n"
     ]
    }
   ],
   "source": [
    "# Let's see the results:\n",
    "!head result.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do It Yourself\n",
    "\n",
    "1. get some CSV data: https://grouplens.org/datasets/movielens/\n",
    "1. register new data as a multi-column CSV table\n",
    "1. find total numbers of movies and users\n",
    "1. find movies viewed by most users and vice versa\n",
    "1. find average numbers of users per movie and vice versa\n",
    "1. find totals of users, movies and averages per year and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
