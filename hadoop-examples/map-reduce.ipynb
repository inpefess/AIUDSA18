{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Streaming\n",
    "\n",
    "Usually MapReduce jobs are written in Java. Nevertheless, Hadoop has a feature called somewhat misleadingly [Hadoop Streaming](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html) which enables one to use Python or any other script language such as `shell` for developing mappers and reducers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Some Code\n",
    "\n",
    "First, we need to code our mapper. In case of Hadoop Streaming, a mapper is a script which gets some text from the standard input until the EOF and produces some text line by line to the standard output. For example, it can be like the following file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/python3\n",
      "\n",
      "counter = 0\n",
      "while True:\n",
      "    try:\n",
      "        counter += 1\n",
      "        input()\n",
      "    except EOFError:\n",
      "        break\n",
      "print(counter)\n"
     ]
    }
   ],
   "source": [
    "!cat mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mind the first line (so-called [shebang](https://en.wikipedia.org/wiki/Shebang_(Unix))) - it's very important to keep it in place, since Hadoop doesn't know where your favourite Python executable is located. It could even be `#!/usr/bin/perl` or `#!/usr/bin/bash` as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script does nothing interesting, it simply goes through the file line by line until EOF and counts lines. Then it prints the total number of lines in a file.\n",
    "\n",
    "Of course, you can code anything more complicated: import additional packages, define functions, classes, and so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reducer looks similar since generally it does the same trick: goes through the lines of the standard input and prints something to the standard output. The main difference is that it has the output of the mapper as it's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/python3\n",
      "\n",
      "counter = 0\n",
      "while True:\n",
      "    try:\n",
      "        line = input()\n",
      "    except EOFError:\n",
      "        break\n",
      "    counter += int(line)\n",
      "print(counter)\n"
     ]
    }
   ],
   "source": [
    "!cat reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reducer sums integer values (which are the line counts produced by the mapper)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pushing your code to the cluster\n",
    "\n",
    "Hadoop lives on a cluster, and your MapReduce jobs will run on the cluster too. Hadoop __can't__ execute any code from a local machine directly. That means we need to put out code to HDFS somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put mapper.py code\n",
    "!hdfs dfs -put reducer.py code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to do that whenever your want to update your MapReduce jobs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running MapReduce\n",
    "\n",
    "We will use the `mapred streaming` command for running our Hadoop Streaming job. The description of parameters follows:\n",
    "* files - here we put a comma-separated list of our source code files __on HDFS__. In case of Python they are simple Python scripts but in case of Java they would be `jar` files\n",
    "* input - a file on HDFS to input to the mapper\n",
    "* output - some location to HDFS where to put the results (the output of the reducer)\n",
    "* mapper - the name of the mapper script\n",
    "* reducer - the name of the reducer script\n",
    "\n",
    "Let magic happen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/home/boris/opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar] /tmp/streamjob3635668951347712675.jar tmpDir=null\n",
      "2020-10-19 17:56:44,237 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-10-19 17:56:44,503 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-10-19 17:56:44,747 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2020-10-19 17:56:44,763 INFO mapreduce.JobSubmitter: number of splits:8\n",
      "2020-10-19 17:56:44,879 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1603104554930_0019\n",
      "2020-10-19 17:56:44,880 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2020-10-19 17:56:45,110 INFO conf.Configuration: resource-types.xml not found\n",
      "2020-10-19 17:56:45,111 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2020-10-19 17:56:45,181 INFO impl.YarnClientImpl: Submitted application application_1603104554930_0019\n",
      "2020-10-19 17:56:45,227 INFO mapreduce.Job: The url to track the job: http://math11.unice.fr:8088/proxy/application_1603104554930_0019/\n",
      "2020-10-19 17:56:45,229 INFO mapreduce.Job: Running job: job_1603104554930_0019\n",
      "2020-10-19 17:56:50,301 INFO mapreduce.Job: Job job_1603104554930_0019 running in uber mode : false\n",
      "2020-10-19 17:56:50,304 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2020-10-19 17:56:56,374 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "2020-10-19 17:56:57,380 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "2020-10-19 17:56:58,385 INFO mapreduce.Job:  map 38% reduce 0%\n",
      "2020-10-19 17:56:59,391 INFO mapreduce.Job:  map 63% reduce 0%\n",
      "2020-10-19 17:57:00,395 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "2020-10-19 17:57:01,401 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2020-10-19 17:57:01,415 INFO mapreduce.Job: Job job_1603104554930_0019 completed successfully\n",
      "2020-10-19 17:57:01,560 INFO mapreduce.Job: Counters: 48\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=265641068\n",
      "\t\tFILE: Number of bytes written=2063340\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=8\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=8\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18846\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1932\n",
      "\t\tTotal time spent by all map tasks (ms)=18846\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1932\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=18846\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1932\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=19298304\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1978368\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1320761\n",
      "\t\tMap output records=8\n",
      "\t\tMap output bytes=64\n",
      "\t\tMap output materialized bytes=128\n",
      "\t\tInput split bytes=1184\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=8\n",
      "\t\tReduce shuffle bytes=128\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=16\n",
      "\t\tShuffled Maps =8\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=8\n",
      "\t\tGC time elapsed (ms)=106\n",
      "\t\tCPU time spent (ms)=14190\n",
      "\t\tPhysical memory (bytes) snapshot=2879238144\n",
      "\t\tVirtual memory (bytes) snapshot=24352129024\n",
      "\t\tTotal committed heap usage (bytes)=7417626624\n",
      "\t\tPeak Map Physical memory (bytes)=344121344\n",
      "\t\tPeak Map Virtual memory (bytes)=2713444352\n",
      "\t\tPeak Reduce Physical memory (bytes)=311291904\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2708545536\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=265633854\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=21\n",
      "2020-10-19 17:57:01,561 INFO streaming.StreamJob: Output directory: data/result\n"
     ]
    }
   ],
   "source": [
    "!mapred streaming \\\n",
    "    -files code/mapper.py,code/reducer.py \\\n",
    "    -input data/yelp_academic_dataset_tip.json \\\n",
    "    -output data/result \\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 boris atg          0 2020-10-19 17:53 data/result/_SUCCESS\n",
      "-rw-r--r--   1 boris atg          9 2020-10-19 17:53 data/result/part-00000\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls data/result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mind the file \\_SUCCESS. It appears only when a MapReduce job finished successfully. Since the reducer outputs a file, here it is - `part-00000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1320769\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat data/result/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the number of lines in our input file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Parts\n",
    "\n",
    "Let's run the same MapReduce job but without a reducer (then it's considered to act as identity and print exactly what it reads from it's input):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/home/boris/opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar] /tmp/streamjob7977458957710831605.jar tmpDir=null\n",
      "2020-10-19 18:02:29,774 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-10-19 18:02:30,047 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-10-19 18:02:30,478 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2020-10-19 18:02:30,495 INFO mapreduce.JobSubmitter: number of splits:8\n",
      "2020-10-19 18:02:30,615 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1603104554930_0020\n",
      "2020-10-19 18:02:30,616 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2020-10-19 18:02:30,843 INFO conf.Configuration: resource-types.xml not found\n",
      "2020-10-19 18:02:30,844 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2020-10-19 18:02:30,913 INFO impl.YarnClientImpl: Submitted application application_1603104554930_0020\n",
      "2020-10-19 18:02:30,948 INFO mapreduce.Job: The url to track the job: http://math11.unice.fr:8088/proxy/application_1603104554930_0020/\n",
      "2020-10-19 18:02:30,950 INFO mapreduce.Job: Running job: job_1603104554930_0020\n",
      "2020-10-19 18:02:37,036 INFO mapreduce.Job: Job job_1603104554930_0020 running in uber mode : false\n",
      "2020-10-19 18:02:37,039 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2020-10-19 18:02:43,121 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "2020-10-19 18:02:44,128 INFO mapreduce.Job:  map 38% reduce 0%\n",
      "2020-10-19 18:02:45,133 INFO mapreduce.Job:  map 63% reduce 0%\n",
      "2020-10-19 18:02:46,138 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "2020-10-19 18:02:47,143 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2020-10-19 18:02:48,150 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2020-10-19 18:02:48,163 INFO mapreduce.Job: Job job_1603104554930_0020 completed successfully\n",
      "2020-10-19 18:02:48,309 INFO mapreduce.Job: Counters: 48\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=265641068\n",
      "\t\tFILE: Number of bytes written=2056915\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=8\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=8\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18965\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1851\n",
      "\t\tTotal time spent by all map tasks (ms)=18965\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1851\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=18965\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1851\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=19420160\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1895424\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1320761\n",
      "\t\tMap output records=8\n",
      "\t\tMap output bytes=64\n",
      "\t\tMap output materialized bytes=128\n",
      "\t\tInput split bytes=1184\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=8\n",
      "\t\tReduce shuffle bytes=128\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=8\n",
      "\t\tSpilled Records=16\n",
      "\t\tShuffled Maps =8\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=8\n",
      "\t\tGC time elapsed (ms)=109\n",
      "\t\tCPU time spent (ms)=13150\n",
      "\t\tPhysical memory (bytes) snapshot=2837573632\n",
      "\t\tVirtual memory (bytes) snapshot=24347856896\n",
      "\t\tTotal committed heap usage (bytes)=7417626624\n",
      "\t\tPeak Map Physical memory (bytes)=343126016\n",
      "\t\tPeak Map Virtual memory (bytes)=2717671424\n",
      "\t\tPeak Reduce Physical memory (bytes)=306929664\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2709823488\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=265633854\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=76\n",
      "2020-10-19 18:02:48,310 INFO streaming.StreamJob: Output directory: data/multi_result\n"
     ]
    }
   ],
   "source": [
    "!mapred streaming \\\n",
    "    -files code/mapper.py,code/reducer.py \\\n",
    "    -input data/yelp_academic_dataset_tip.json \\\n",
    "    -output data/multi_result \\\n",
    "    -mapper mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 boris atg          0 2020-10-19 17:59 data/multi_result/_SUCCESS\n",
      "-rw-r--r--   1 boris atg         64 2020-10-19 17:59 data/multi_result/part-00000\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls data/multi_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there is only one resulting file, it has several lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144176\t\n",
      "165905\t\n",
      "166856\t\n",
      "167410\t\n",
      "167537\t\n",
      "169365\t\n",
      "169534\t\n",
      "169986\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat data/multi_result/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That happened because our mapper was run in parallel. You can wonder why it was slow then. The answer is quite simple - Python is an interpreted language, so it would be faster if we were using C++ or Java for our mappers and reducers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do It Yourself\n",
    "* code a mapper for counting words or characters, not lines\n",
    "* code a reducer to count lines, words, and symbols in one job\n",
    "* code a mapper to sum over the `compliment_count` field value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
