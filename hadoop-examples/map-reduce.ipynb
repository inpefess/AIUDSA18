{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Streaming\n",
    "\n",
    "Usually MapReduce jobs are written in Java. Nevertheless, Hadoop has a feature called somewhat misleadingly [Hadoop Streaming](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html) which enables one to use Python or any other script language such as `shell` for developing mappers and reducers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Some Code\n",
    "\n",
    "First, we need to code our mapper. In case of Hadoop Streaming, a mapper is a script which gets some text from the standard input until the EOF and produces some text line by line to the standard output. For example, it can be like the following file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/python\r\n",
      "\r\n",
      "counter = 0\r\n",
      "while True:\r\n",
      "    try:\r\n",
      "        counter += 1\r\n",
      "        input()\r\n",
      "    except EOFError:\r\n",
      "        break\r\n",
      "print(counter)\r\n"
     ]
    }
   ],
   "source": [
    "!cat /home/borisshminke/Downloads/mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mind the first line (so-called [shebang](https://en.wikipedia.org/wiki/Shebang_(Unix))) - it's very important to keep it in place, since Hadoop doesn't know where your favourite Python executable is located. It could even be `#!/usr/bin/perl` or `#!/usr/bin/bash` as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script does nothing interesting, it simply goes through the file line by line until EOF and counts lines. Then it prints the total number of lines in a file.\n",
    "\n",
    "Of course, you can code anything more complicated: import additional packages, define functions, classes, and so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reducer looks similar since generally it does the same trick: goes through the lines of the standard input and prints something to the standard output. The main difference is that it has the output of the mapper as it's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/python\r\n",
      "\r\n",
      "counter = 0\r\n",
      "while True:\r\n",
      "    try:\r\n",
      "        line = input()\r\n",
      "    except EOFError:\r\n",
      "        break\r\n",
      "    counter += int(line)\r\n",
      "print(counter)\r\n"
     ]
    }
   ],
   "source": [
    "!cat /home/borisshminke/Downloads/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reducer sums integer values (which are the line counts produced by the mapper)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pushing your code to the cluster\n",
    "\n",
    "Hadoop lives on a cluster, and your MapReduce jobs will run on the cluster too. Hadoop __can't__ execute any code from a local machine directly. That means we need to put out code to HDFS somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /user/borisshminke/code\n",
    "!hdfs dfs -put \\\n",
    "    file:///home/borisshminke/Downloads/mapper.py \\\n",
    "    /user/borisshminke/code\n",
    "!hdfs dfs -put \\\n",
    "    file:///home/borisshminke/Downloads/reducer.py \\\n",
    "    /user/borisshminke/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 root hadoop        139 2020-11-17 09:26 /user/borisshminke/code/mapper.py\r\n",
      "-rw-r--r--   1 root hadoop        150 2020-11-17 09:26 /user/borisshminke/code/reducer.py\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/borisshminke/code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to do that whenever your want to update your MapReduce jobs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running MapReduce\n",
    "\n",
    "We will use the `mapred streaming` command for running our Hadoop Streaming job. The description of parameters follows:\n",
    "* files - here we put a comma-separated list of our source code files __on HDFS__. In case of Python they are simple Python scripts but in case of Java they would be `jar` files\n",
    "* input - a file on HDFS to input to the mapper\n",
    "* output - some location to HDFS where to put the results (the output of the reducer)\n",
    "* mapper - the name of the mapper script\n",
    "* reducer - the name of the reducer script\n",
    "\n",
    "Let magic happen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.9.2.jar] /tmp/streamjob3531300907387079280.jar tmpDir=null\n",
      "20/11/17 09:26:44 INFO client.RMProxy: Connecting to ResourceManager at cluster-df2a-m/10.164.0.3:8032\n",
      "20/11/17 09:26:44 INFO client.AHSProxy: Connecting to Application History server at cluster-df2a-m/10.164.0.3:10200\n",
      "20/11/17 09:26:44 INFO client.RMProxy: Connecting to ResourceManager at cluster-df2a-m/10.164.0.3:8032\n",
      "20/11/17 09:26:44 INFO client.AHSProxy: Connecting to Application History server at cluster-df2a-m/10.164.0.3:10200\n",
      "20/11/17 09:26:45 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "20/11/17 09:26:45 INFO mapreduce.JobSubmitter: number of splits:48\n",
      "20/11/17 09:26:45 INFO Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled\n",
      "20/11/17 09:26:45 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1605604180264_0001\n",
      "20/11/17 09:26:46 INFO impl.YarnClientImpl: Submitted application application_1605604180264_0001\n",
      "20/11/17 09:26:46 INFO mapreduce.Job: The url to track the job: http://cluster-df2a-m:8088/proxy/application_1605604180264_0001/\n",
      "20/11/17 09:26:46 INFO mapreduce.Job: Running job: job_1605604180264_0001\n",
      "20/11/17 09:26:54 INFO mapreduce.Job: Job job_1605604180264_0001 running in uber mode : false\n",
      "20/11/17 09:26:54 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "20/11/17 09:27:14 INFO mapreduce.Job:  map 3% reduce 0%\n",
      "20/11/17 09:27:16 INFO mapreduce.Job:  map 6% reduce 0%\n",
      "20/11/17 09:27:35 INFO mapreduce.Job:  map 10% reduce 0%\n",
      "20/11/17 09:27:36 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "20/11/17 09:27:55 INFO mapreduce.Job:  map 17% reduce 0%\n",
      "20/11/17 09:27:56 INFO mapreduce.Job:  map 19% reduce 0%\n",
      "20/11/17 09:28:14 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "20/11/17 09:28:31 INFO mapreduce.Job:  map 29% reduce 0%\n",
      "20/11/17 09:28:32 INFO mapreduce.Job:  map 31% reduce 0%\n",
      "20/11/17 09:28:48 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "20/11/17 09:28:49 INFO mapreduce.Job:  map 38% reduce 0%\n",
      "20/11/17 09:29:06 INFO mapreduce.Job:  map 39% reduce 0%\n",
      "20/11/17 09:29:07 INFO mapreduce.Job:  map 41% reduce 0%\n",
      "20/11/17 09:29:08 INFO mapreduce.Job:  map 44% reduce 0%\n",
      "20/11/17 09:29:24 INFO mapreduce.Job:  map 46% reduce 0%\n",
      "20/11/17 09:29:26 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "20/11/17 09:29:41 INFO mapreduce.Job:  map 51% reduce 0%\n",
      "20/11/17 09:29:42 INFO mapreduce.Job:  map 52% reduce 0%\n",
      "20/11/17 09:29:44 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "20/11/17 09:29:45 INFO mapreduce.Job:  map 56% reduce 0%\n",
      "20/11/17 09:29:59 INFO mapreduce.Job:  map 57% reduce 0%\n",
      "20/11/17 09:30:01 INFO mapreduce.Job:  map 58% reduce 0%\n",
      "20/11/17 09:30:04 INFO mapreduce.Job:  map 63% reduce 0%\n",
      "20/11/17 09:30:19 INFO mapreduce.Job:  map 65% reduce 0%\n",
      "20/11/17 09:30:23 INFO mapreduce.Job:  map 69% reduce 0%\n",
      "20/11/17 09:30:37 INFO mapreduce.Job:  map 71% reduce 0%\n",
      "20/11/17 09:30:40 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "20/11/17 09:30:54 INFO mapreduce.Job:  map 77% reduce 0%\n",
      "20/11/17 09:30:57 INFO mapreduce.Job:  map 79% reduce 0%\n",
      "20/11/17 09:30:58 INFO mapreduce.Job:  map 81% reduce 0%\n",
      "20/11/17 09:31:10 INFO mapreduce.Job:  map 82% reduce 0%\n",
      "20/11/17 09:31:13 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "20/11/17 09:31:15 INFO mapreduce.Job:  map 85% reduce 0%\n",
      "20/11/17 09:31:16 INFO mapreduce.Job:  map 88% reduce 0%\n",
      "20/11/17 09:31:30 INFO mapreduce.Job:  map 89% reduce 0%\n",
      "20/11/17 09:31:31 INFO mapreduce.Job:  map 90% reduce 0%\n",
      "20/11/17 09:31:32 INFO mapreduce.Job:  map 91% reduce 0%\n",
      "20/11/17 09:31:33 INFO mapreduce.Job:  map 92% reduce 0%\n",
      "20/11/17 09:31:34 INFO mapreduce.Job:  map 94% reduce 0%\n",
      "20/11/17 09:31:44 INFO mapreduce.Job:  map 96% reduce 0%\n",
      "20/11/17 09:31:46 INFO mapreduce.Job:  map 98% reduce 0%\n",
      "20/11/17 09:31:50 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "20/11/17 09:31:51 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "20/11/17 09:31:54 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "20/11/17 09:31:55 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "20/11/17 09:31:56 INFO mapreduce.Job: Job job_1605604180264_0001 completed successfully\n",
      "20/11/17 09:31:56 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=497\n",
      "\t\tFILE: Number of bytes written=10785876\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6325763976\n",
      "\t\tHDFS: Number of bytes written=27\n",
      "\t\tHDFS: Number of read operations=159\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=48\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=48\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=2462697\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=46314\n",
      "\t\tTotal time spent by all map tasks (ms)=820899\n",
      "\t\tTotal time spent by all reduce tasks (ms)=15438\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=820899\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=15438\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=2521801728\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=47425536\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=8021122\n",
      "\t\tMap output records=48\n",
      "\t\tMap output bytes=383\n",
      "\t\tMap output materialized bytes=1343\n",
      "\t\tInput split bytes=6240\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=48\n",
      "\t\tReduce shuffle bytes=1343\n",
      "\t\tReduce input records=48\n",
      "\t\tReduce output records=3\n",
      "\t\tSpilled Records=96\n",
      "\t\tShuffled Maps =144\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=144\n",
      "\t\tGC time elapsed (ms)=8183\n",
      "\t\tCPU time spent (ms)=432040\n",
      "\t\tPhysical memory (bytes) snapshot=27628867584\n",
      "\t\tVirtual memory (bytes) snapshot=222876975104\n",
      "\t\tTotal committed heap usage (bytes)=24549785600\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=6325757736\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=27\n",
      "20/11/17 09:31:56 INFO streaming.StreamJob: Output directory: /user/borisshminke/data/result\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming-2.9.2.jar \\\n",
    "    -files hdfs:///user/borisshminke/code/mapper.py,hdfs:///user/borisshminke/code/reducer.py \\\n",
    "    -input /user/borisshminke/data/yelp_academic_dataset_review.json \\\n",
    "    -output /user/borisshminke/data/result \\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   1 root hadoop          0 2020-11-17 09:31 /user/borisshminke/data/result/_SUCCESS\r\n",
      "-rw-r--r--   1 root hadoop          9 2020-11-17 09:31 /user/borisshminke/data/result/part-00000\r\n",
      "-rw-r--r--   1 root hadoop          9 2020-11-17 09:31 /user/borisshminke/data/result/part-00001\r\n",
      "-rw-r--r--   1 root hadoop          9 2020-11-17 09:31 /user/borisshminke/data/result/part-00002\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/borisshminke/data/result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mind the file \\_SUCCESS. It appears only when a MapReduce job finished successfully. Since the reducer outputs files, here they are - `part-0000*`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3247036\t\r\n",
      "2044848\t\r\n",
      "2729286\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/borisshminke/data/result/part-0000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the numbers of lines in our input file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That happened because our mapper was run in parallel. You can wonder why it was slow then. The answer is quite simple - Python is an interpreted language, so it would be faster if we were using C++ or Java for our mappers and reducers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do It Yourself\n",
    "* code a mapper for counting characters, not lines\n",
    "* code a reducer to count lines and characters in one job\n",
    "* code a mapper to sum over the `compliment_count` field value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "name": "map-reduce.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
