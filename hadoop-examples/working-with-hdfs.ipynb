{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Know your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRETTY_NAME=\"Debian GNU/Linux 10 (buster)\"\r\n",
      "NAME=\"Debian GNU/Linux\"\r\n",
      "VERSION_ID=\"10\"\r\n",
      "VERSION=\"10 (buster)\"\r\n",
      "VERSION_CODENAME=buster\r\n",
      "ID=debian\r\n",
      "HOME_URL=\"https://www.debian.org/\"\r\n",
      "SUPPORT_URL=\"https://www.debian.org/support\"\r\n",
      "BUG_REPORT_URL=\"https://bugs.debian.org/\"\r\n"
     ]
    }
   ],
   "source": [
    "!cat /etc/os-release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 2.7.14 :: Anaconda, Inc.\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop 2.9.2\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop version | head -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with folders on HDFS\n",
    "It's quite similar to working with folders on any \\*nix system, just don't forget to add `hdfs dfs -` before common commands:\n",
    "* `ls`\n",
    "* `mkdir`\n",
    "* `rmdir`\n",
    "* `mv`\n",
    "\n",
    "Usually it's a good idea to use your user's directory: `/uesr/%username%`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/borisshminke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /user/borisshminke/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/borisshminke/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /user/borisshminke/data/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "drwxr-xr-x   - root hadoop          0 2020-11-17 09:21 /user/borisshminke/data/tmp\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/borisshminke/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mv /user/borisshminke/data/tmp /user/borisshminke/data/tmp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - root hadoop          0 2020-11-17 09:21 /user/borisshminke/data/tmp1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/borisshminke/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -rmdir /user/borisshminke/data/tmp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/borisshminke/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mind the difference between you local system and HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin   dev  hadoop  lib\t  lib64   lost+found  mnt  proc  run   srv  tmp  var\r\n",
      "boot  etc  home    lib32  libx32  media       opt  root  sbin  sys  usr\r\n"
     ]
    }
   ],
   "source": [
    "!ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Found 3 items',\n",
       " 'drwx------   - mapred hadoop          0 2020-11-17 09:10 /hadoop',\n",
       " 'drwxrwxrwt   - hdfs   hadoop          0 2020-11-17 09:10 /tmp',\n",
       " 'drwxrwxrwt   - hdfs   hadoop          0 2020-11-17 09:10 /user']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading data from your local filesystem to HDFS and getting it back\n",
    "For uploading and downloading data from your HDFS cluster, an `ftp`-like parlance is used:\n",
    "* `put`\n",
    "* `get`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put \\\n",
    "    file:///home/borisshminke/Downloads/yelp_academic_dataset_review.json \\\n",
    "    /user/borisshminke/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `du` command to measure the actual size of your data on HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.9 G  /user/borisshminke/data/yelp_academic_dataset_review.json\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -du -h /user/borisshminke/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/borisshminke/Downloads/from_hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -get /user/borisshminke/data/* file:///home/borisshminke/Downloads/from_hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 5.9G\n",
      "-rw-r--r-- 1 root root 5.9G Nov 17 09:24 yelp_academic_dataset_review.json\n"
     ]
    }
   ],
   "source": [
    "!ls -lh /home/borisshminke/Downloads/from_hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mind that since storage on HDFS is distributed, you can end up `get`-ing a bunch of files instead of one, each slice coming from a different node of the cluster.\n",
    "\n",
    "If you want to get a single file, use `getmerge` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have several HDFS clusters in your disposal, you can move data from one to another with the same pair of commands (`get`/`put`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do It Yourself\n",
    "* run `hdfs dfs --help` and get an idea of other possible commands\n",
    "* try to create a subfolder in your `/user/%username%` folder\n",
    "* create a new file on HDFS using `touch`\n",
    "* `cat` the newly created file (don't do that in future when working with huge files:))\n",
    "* upload some files from your system to the cluster\n",
    "* use `head`/`tail` to look into the uploaded files\n",
    "* create a full copy of your working subfolder on HDFS (use `cp`)\n",
    "* remove the redundant copy\n",
    "* compare the results of `get` and `getmerge` commands\n",
    "* compare the results of `put` and `moveFromLocal` commands\n",
    "* feel free to play with other HDFS commands"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "name": "working-with-hdfs.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
