{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code was tested in a particular environment\n",
    "These examples were tested in a pseudo-distributed installation of Hadoop ([see more](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME=\"CentOS Linux\"\n",
      "VERSION=\"7 (Core)\"\n",
      "ID=\"centos\"\n",
      "ID_LIKE=\"rhel fedora\"\n",
      "VERSION_ID=\"7\"\n",
      "PRETTY_NAME=\"CentOS Linux 7 (Core)\"\n",
      "ANSI_COLOR=\"0;31\"\n",
      "CPE_NAME=\"cpe:/o:centos:centos:7\"\n",
      "HOME_URL=\"https://www.centos.org/\"\n",
      "BUG_REPORT_URL=\"https://bugs.centos.org/\"\n",
      "\n",
      "CENTOS_MANTISBT_PROJECT=\"CentOS-7\"\n",
      "CENTOS_MANTISBT_PROJECT_VERSION=\"7\"\n",
      "REDHAT_SUPPORT_PRODUCT=\"centos\"\n",
      "REDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat /etc/os-release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.0\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop 3.2.1\n"
     ]
    }
   ],
   "source": [
    "!hadoop version | head -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with folders on HDFS\n",
    "It's quite similar to working with folders on any \\*nix system, just don't forget to add `hdfs dfs -` before common commands:\n",
    "* `ls`\n",
    "* `mkdir`\n",
    "* `rmdir`\n",
    "* `mv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir data/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - boris atg          2 2020-10-19 13:05 data/tmp\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mv data/tmp data/tmp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - boris atg          2 2020-10-19 13:05 data/tmp1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -rmdir data/tmp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading data from your local filesystem to HDFS and getting it back\n",
    "For uploading and downloading data from your HDFS cluster, an `ftp`-like parlance is used:\n",
    "* `put`\n",
    "* `get`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put file:///home/boris/Downloads/yelp_dataset/yelp_academic_dataset_tip.json data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `du` command to measure the actual size of your data on HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251.3 M  251.3 M  data/yelp_academic_dataset_tip.json\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -du -h data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -get data/* file:///home/boris/Downloads/from_hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yelp_academic_dataset_tip.json\n"
     ]
    }
   ],
   "source": [
    "!ls ~/Downloads/from_hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mind that since storage on HDFS is distributed, you can end up `get`-ing a bunch of files instead of one, each slice coming from a different node of the cluster.\n",
    "\n",
    "If you want to get a single file, use `getmerge` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get rid of data which you don't need anymore, you can use an good old `rm`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-19 13:36:43,509 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
      "Deleted data/yelp_academic_dataset_tip.json\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm data/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -du -h data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have several HDFS clusters in your disposal, you can move data from one to another with the same pair of commands (`get`/`put`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do It Yourself\n",
    "* run `hdfs dfs --help` and get an idea of other possible commands\n",
    "* try to create a subfolder in your `/user/%username%` folder\n",
    "* create a new file on HDFS using `touch`\n",
    "* `cat` the newly created file (don't do that in future when working with huge files:))\n",
    "* upload some files from your system to the cluster\n",
    "* use `head`/`tail` to look into the uploaded files\n",
    "* create a full copy of your working subfolder on HDFS (use `cp`)\n",
    "* remove the redundant copy\n",
    "* compare the results of `get` and `getmerge` commands\n",
    "* compare the results of `put` and `moveFromLocal` commands\n",
    "* feel free to play with other HDFS commands"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
