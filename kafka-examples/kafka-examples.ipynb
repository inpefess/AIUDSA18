{"cells":[{"cell_type":"markdown","source":["# Python Kafka\n\nThere exists a neat Python package for communicating with a running Kafka cluster. You can even admin the cluster using it:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2cbaef41-9552-4c1b-939a-bc75ba3f696b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["!pip install kafka-python"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"36b2882f-4404-435c-8ecb-2cab49520368","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Collecting kafka-python\r\n  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\r\n\u001B[?25l\r\u001B[K     |█▎                              | 10 kB 22.0 MB/s eta 0:00:01\r\u001B[K     |██▋                             | 20 kB 3.6 MB/s eta 0:00:01\r\u001B[K     |████                            | 30 kB 5.3 MB/s eta 0:00:01\r\u001B[K     |█████▎                          | 40 kB 4.3 MB/s eta 0:00:01\r\u001B[K     |██████▋                         | 51 kB 4.3 MB/s eta 0:00:01\r\u001B[K     |████████                        | 61 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |█████████▎                      | 71 kB 4.9 MB/s eta 0:00:01\r\u001B[K     |██████████▋                     | 81 kB 4.7 MB/s eta 0:00:01\r\u001B[K     |████████████                    | 92 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████████▎                  | 102 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |██████████████▋                 | 112 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |████████████████                | 122 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |█████████████████▎              | 133 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |██████████████████▋             | 143 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |████████████████████            | 153 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |█████████████████████▎          | 163 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |██████████████████████▋         | 174 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |████████████████████████        | 184 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████▎      | 194 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████▋     | 204 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████    | 215 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████████▎  | 225 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████████▋ | 235 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 245 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 246 kB 5.1 MB/s \r\n\u001B[?25hInstalling collected packages: kafka-python\r\nSuccessfully installed kafka-python-2.0.2\r\n\u001B[33mWARNING: You are using pip version 21.0.1; however, version 22.3.1 is available.\r\nYou should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Collecting kafka-python\r\n  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\r\n\u001B[?25l\r\u001B[K     |█▎                              | 10 kB 22.0 MB/s eta 0:00:01\r\u001B[K     |██▋                             | 20 kB 3.6 MB/s eta 0:00:01\r\u001B[K     |████                            | 30 kB 5.3 MB/s eta 0:00:01\r\u001B[K     |█████▎                          | 40 kB 4.3 MB/s eta 0:00:01\r\u001B[K     |██████▋                         | 51 kB 4.3 MB/s eta 0:00:01\r\u001B[K     |████████                        | 61 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |█████████▎                      | 71 kB 4.9 MB/s eta 0:00:01\r\u001B[K     |██████████▋                     | 81 kB 4.7 MB/s eta 0:00:01\r\u001B[K     |████████████                    | 92 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████████▎                  | 102 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |██████████████▋                 | 112 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |████████████████                | 122 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |█████████████████▎              | 133 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |██████████████████▋             | 143 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |████████████████████            | 153 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |█████████████████████▎          | 163 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |██████████████████████▋         | 174 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |████████████████████████        | 184 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████▎      | 194 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████▋     | 204 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████    | 215 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████████▎  | 225 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████████▋ | 235 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 245 kB 5.1 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 246 kB 5.1 MB/s \r\n\u001B[?25hInstalling collected packages: kafka-python\r\nSuccessfully installed kafka-python-2.0.2\r\n\u001B[33mWARNING: You are using pip version 21.0.1; however, version 22.3.1 is available.\r\nYou should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Kafka cluster\n\nTo use `kafka-python`, you need a Kafka cluster running somewhere.\n\nYou can create a Kafka cluster using [Confluent on Google Cloud](https://console.cloud.google.com/marketplace/product/confluent-prod/apache-kafka-on-confluent-cloud), paying with your Google Cloud credits for education.\n\nIn class, the instructor will provide you with the credentials to connect to an existing cluster."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"da02484b-e8ec-480a-8d92-b17af87a023d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# cluster connection parameters\nparams = {\n    \"bootstrap_servers\": \"put the bootstrap server address here\",\n    \"security_protocol\": \"SASL_SSL\",\n    \"sasl_mechanism\": \"PLAIN\",\n    \"sasl_plain_username\": \"use API key as username\",\n    \"sasl_plain_password\": \"use API secret as password\"\n}"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1acc85f1-dfa1-4975-8dcc-8b4f6eb77736","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# use another name to avoid conflicts with other students when in class\n\nMAIN_TOPIC = \"main_topic\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a07d8e60-c6be-4882-bf0e-d5e6c6c60006","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from kafka import KafkaAdminClient\n\nadmin_client = KafkaAdminClient(**params)\n# with AdminClient you can do anything with Kafka\nprint(admin_client.list_topics())\nadmin_client.delete_topics(admin_client.list_topics())\nprint(admin_client.list_topics())"],"metadata":{"scrolled":true,"tags":[],"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5e830d62-f401-4180-8d10-6c17f15e3777","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[]\n[]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[]\n[]\n"]}}],"execution_count":0},{"cell_type":"code","source":["from kafka.admin import NewTopic\n\nprint(admin_client.list_topics())\nadmin_client.create_topics([\n    NewTopic(\n        name=MAIN_TOPIC,\n        # we won't use topic partitioning for this example\n        # thus leaving the number of partitions to 1\n        num_partitions=1,\n        # replication_factor is defined by cluster's configuration\n        replication_factor=3,\n        # the topic will start discarding old data when it becomes larger than that\n        topic_configs={\"retention.bytes\": 2 ** 30}\n    )\n])\nprint(admin_client.list_topics())"],"metadata":{"scrolled":true,"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8061a5ff-68f4-4659-b858-afe2dea34140","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[]\n['main_topic']\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[]\n['main_topic']\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Writing Data to Kafka\n\nMind that one can write only `bytes` to Kafka, not strings!"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"066b39a5-c83c-47a4-b6ee-42f8aff01b1f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from kafka import KafkaProducer\n\nproducer = KafkaProducer(**params)\nfor i in range(10):\n    producer.send(MAIN_TOPIC, str(i).encode(\"utf-8\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6363e415-c8cd-40e4-99df-c9cbf24f5739","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Reading Data from Kafka\n\nReading is done with a consumer"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ecda082e-44e5-4ed0-960d-3675ba5c3cfe","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from kafka import KafkaConsumer\n\nconsumer = KafkaConsumer(**params)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d30f3449-ab51-45f7-a8e4-949d3b7047aa","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Topics, Partitions, and Offsets\n\nMessages in Kafka are organised into topics:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"733c1f49-44ad-4a69-86d1-dccc55c00106","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["consumer.topics()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e9387b53-9437-462e-9e26-145e80140b13","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[8]: {'main_topic'}","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[8]: {'main_topic'}"]}}],"execution_count":0},{"cell_type":"markdown","source":["Topic can have several partitions with different starting and ending offsets:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"27ab1273-5825-4da3-aa7f-56ed62ea8109","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from kafka import TopicPartition\n\npartitions = [\n    TopicPartition(MAIN_TOPIC, partition)\n    for partition in consumer.partitions_for_topic(MAIN_TOPIC)\n]\nprint(consumer.beginning_offsets(partitions))\nprint(consumer.end_offsets(partitions))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"301367aa-9db5-4e6a-8b38-a7dbddd91a10","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"{TopicPartition(topic='main_topic', partition=0): 0}\n{TopicPartition(topic='main_topic', partition=0): 10}\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["{TopicPartition(topic='main_topic', partition=0): 0}\n{TopicPartition(topic='main_topic', partition=0): 10}\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Before reading something from Kafka, one should assign the consumer to a topic and partition:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0561c647-6947-4706-9531-9d0018c1f665","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# Reading from a given offset of a partition"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f0f8624b-8bb6-4c4e-aee4-c87e37c615da","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# before consuming data, one needs to assign partitions\nconsumer.assign(partitions)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"34e40c4a-f9e0-4e9d-a758-8dd9206de69d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["One can read from any offset of the partition:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"76cb4479-0b89-45c2-a39c-484abbdbaabc","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# you can check, at which offset the partition is currently read\nprint(consumer.position(partitions[0]))\n# and set the offset as you wish\nconsumer.seek(partitions[0], 0)\nprint(consumer.position(partitions[0]))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"327af3b3-764a-4483-8839-e43686105e3b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"10\n0\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["10\n0\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Reading data can be done by batches of any desired size:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e0ab3b10-ad41-4ee0-aeb4-a91ca0d0466a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["for _ in range(10):\n    data = consumer.poll(\n        timeout_ms=200,\n        max_records=1\n    )[partitions[0]][0].value\n    print(data.decode(\"utf-8\"))\nprint(consumer.position(partitions[0]))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"860bb3d4-3724-4543-a2e6-735c0fc88144","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Do it Yourself\n\nUse three notebooks:\n\n1. write an endless stream of random float numbers to Kafka\n1. work with the stream:\n    1. read from the input topic in batches of 1024 values\n    1. compute their averages\n    1. write the stream of averages to another topic\n1. read from the output topic to verify the results"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"aa31f6a7-c4d9-440e-8449-6531aa4cbbd6","inputWidgets":{},"title":""}}}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.8.12","nbconvert_exporter":"python","file_extension":".py"},"name":"kafka-examples.ipynb","application/vnd.databricks.v1+notebook":{"notebookName":"kafka-examples","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3020591943767465}},"nbformat":4,"nbformat_minor":0}
