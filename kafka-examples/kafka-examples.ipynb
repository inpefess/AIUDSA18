{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Python Kafka\n\nThere exists a neat Python package for communicating with a running Kafka cluster. You can even admin the cluster using it:"}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Requirement already satisfied: kafka-python in /opt/conda/miniconda3/lib/python3.8/site-packages (2.0.2)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"}], "source": "!pip install kafka-python"}, {"cell_type": "markdown", "metadata": {}, "source": "Confluent advertises it's own client, [`confluent-kafka`](https://pypi.org/project/confluent-kafka/)\n\nBut `kafka-python` works with any Kafka installation, not only Confluent"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": "# use Confluent webpage to get that info\nparams = {\n    # take this from \"Cluster Overview->Cluster Settings->General->Identification\"\n    \"bootstrap_servers\": \"pkc-4r297.europe-west1.gcp.confluent.cloud:9092\",\n    \"security_protocol\": \"SASL_SSL\",\n    \"sasl_mechanism\": \"PLAIN\",\n    # Data Integration->API Keys->Add key\n    # username = key\n    \"sasl_plain_username\": \"4KLAOHXKUM6GFMLJ\",\n    # password = secret\n    \"sasl_plain_password\": \"tdpFCiSB+pMISzXXsLnkBeYZAKc9+lb4rIRWcuv7simAlUWUbqRi8KxUt21w5+XY\"\n}"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": "# you can use another name for testing\n\nMAIN_TOPIC = \"main_topic\""}, {"cell_type": "code", "execution_count": 4, "metadata": {"scrolled": true, "tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[]\n[]\n"}], "source": "from kafka import KafkaAdminClient\n\nadmin_client = KafkaAdminClient(**params)\n# with AdminClient you can do anything with Kafka\nprint(admin_client.list_topics())\nadmin_client.delete_topics(admin_client.list_topics())\nprint(admin_client.list_topics())"}, {"cell_type": "code", "execution_count": 5, "metadata": {"scrolled": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[]\n['main_topic']\n"}], "source": "from kafka.admin import NewTopic\n\nprint(admin_client.list_topics())\nadmin_client.create_topics([\n    NewTopic(\n        name=MAIN_TOPIC,\n        # we won't use topic partitioning for this example\n        # thus leaving the number of partitions to 1\n        num_partitions=1,\n        # replication_factor is defined by cluster's configuration\n        replication_factor=3,\n        # the topic will start discarding old data when it becomes larger than that\n        topic_configs={\"retention.bytes\": 2 ** 30}\n    )\n])\nprint(admin_client.list_topics())"}, {"cell_type": "markdown", "metadata": {}, "source": "# Writing Data to Kafka\n\nMind that one can write only `bytes` to Kafka, not strings!"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": "from kafka import KafkaProducer\n\nproducer = KafkaProducer(**params)\nfor i in range(10):\n    producer.send(MAIN_TOPIC, str(i).encode(\"utf-8\"))"}, {"cell_type": "markdown", "metadata": {}, "source": "# Reading Data from Kafka\n\nReading is done with a consumer"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": "from kafka import KafkaConsumer\n\nconsumer = KafkaConsumer(**params)"}, {"cell_type": "markdown", "metadata": {}, "source": "# Topics, Partitions, and Offsets\n\nMessages in Kafka are organised into topics:"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"data": {"text/plain": "{'main_topic'}"}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": "consumer.topics()"}, {"cell_type": "markdown", "metadata": {}, "source": "Topic can have several partitions with different starting and ending offsets:"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "{TopicPartition(topic='main_topic', partition=0): 0}\n{TopicPartition(topic='main_topic', partition=0): 10}\n"}], "source": "from kafka import TopicPartition\n\npartitions = [\n    TopicPartition(MAIN_TOPIC, partition)\n    for partition in consumer.partitions_for_topic(MAIN_TOPIC)\n]\nprint(consumer.beginning_offsets(partitions))\nprint(consumer.end_offsets(partitions))"}, {"cell_type": "markdown", "metadata": {}, "source": "Before reading something from Kafka, one should assign the consumer to a topic and partition:"}, {"cell_type": "markdown", "metadata": {}, "source": "# Reading from a given offset of a partition"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": "# before consuming data, one needs to assign partitions\nconsumer.assign(partitions)"}, {"cell_type": "markdown", "metadata": {}, "source": "One can read from any offset of the partition:"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "10\n0\n"}], "source": "# you can check, at which offset the partition is currently read\nprint(consumer.position(partitions[0]))\n# and set the offset as you wish\nconsumer.seek(partitions[0], 0)\nprint(consumer.position(partitions[0]))"}, {"cell_type": "markdown", "metadata": {}, "source": "Reading data can be done by batches of any desired size:"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n"}], "source": "for _ in range(10):\n    data = consumer.poll(\n        timeout_ms=200,\n        max_records=1\n    )[partitions[0]][0].value\n    print(data.decode(\"utf-8\"))\nprint(consumer.position(partitions[0]))"}, {"cell_type": "markdown", "metadata": {}, "source": "# Do it Yourself\n\n* use two notebooks\n* in the first one, write an endless stream of random float numbers to Kafka\n* in the second one, read the stream in batches of 1024 values,  and compute their averages\n* write the stream of averages to another topic\n* monitor the topics status in Confluent"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.12"}, "name": "kafka-examples.ipynb"}, "nbformat": 4, "nbformat_minor": 4}