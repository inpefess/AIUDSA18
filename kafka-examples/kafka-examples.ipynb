{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Kafka\n",
    "\n",
    "There exists a neat Python package for communicating with a running Kafka cluster. You can even admin the cluster using it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka-python in /opt/conda/anaconda/lib/python2.7/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 20.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"bootstrap_servers\": \"pkc-ewzgj.europe-west4.gcp.confluent.cloud:9092\",\n",
    "    \"security_protocol\": \"SASL_SSL\",\n",
    "    \"sasl_mechanism\": \"PLAIN\",\n",
    "    \"sasl_plain_username\": \"6T3V4SHANAZC5EXK\",\n",
    "    \"sasl_plain_password\": \"CJFtpZwGh/w8mltYpFJRSs2ePW+ho2/7VbwMn4ZarXej23Gm9p1cGgArfsP3o342\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreateTopicsResponse_v3(throttle_time_ms=0, topic_errors=[(topic=u'main_topic', error_code=0, error_message=None)])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kafka import KafkaAdminClient\n",
    "from kafka.admin import NewTopic\n",
    "\n",
    "MAIN_TOPIC = \"main_topic\"\n",
    "admin_client = KafkaAdminClient(**params)\n",
    "# with AdminClient you can do anything with Kafka\n",
    "admin_client.delete_topics(admin_client.list_topics())\n",
    "# replication_factor is defined by cluster's configuration\n",
    "# we won't use topic partitioning for this example\n",
    "# thus leaving the number of partitions to 1\n",
    "admin_client.create_topics([\n",
    "    NewTopic(\n",
    "        name=MAIN_TOPIC,\n",
    "        num_partitions=1,\n",
    "        replication_factor=3\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Data to Kafka\n",
    "\n",
    "Mind that one can write only `bytes` to Kafka, not strings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'main_topic']\n",
      "[u'main_topic']\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "\n",
    "print(admin_client.list_topics())\n",
    "producer = KafkaProducer(**params)\n",
    "for i in range(10):\n",
    "    producer.send(MAIN_TOPIC, bytes(i))\n",
    "print(admin_client.list_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data from Kafka\n",
    "\n",
    "Reading is done with a consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "consumer = KafkaConsumer(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics, Partitions, and Offsets\n",
    "\n",
    "Messages in Kafka are organised into topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'main_topic'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consumer.topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic can have several partitions with different starting and ending offsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{TopicPartition(topic=u'main_topic', partition=0): 0}\n",
      "{TopicPartition(topic=u'main_topic', partition=0): 10}\n"
     ]
    }
   ],
   "source": [
    "from kafka import TopicPartition\n",
    "\n",
    "partitions = [\n",
    "    TopicPartition(MAIN_TOPIC, partition)\n",
    "    for partition in consumer.partitions_for_topic(MAIN_TOPIC)\n",
    "]\n",
    "print(consumer.beginning_offsets(partitions))\n",
    "print(consumer.end_offsets(partitions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before reading something from Kafka, one should assign the consumer to a topic and partition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading from a given offset of a partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer.assign(partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can read from any offset of the partition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(consumer.position(partitions[0]))\n",
    "consumer.seek(partitions[0], 0)\n",
    "print(consumer.position(partitions[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data can be done by batches of any desired size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    data = consumer.poll(\n",
    "        timeout_ms=10,\n",
    "        max_records=1\n",
    "    )[partitions[0]][0].value\n",
    "    print(data)\n",
    "print(consumer.position(partitions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "name": "kafka-examples.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
