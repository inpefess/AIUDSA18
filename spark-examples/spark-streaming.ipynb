{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ":: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"}, {"name": "stderr", "output_type": "stream", "text": "Ivy Default Cache set to: /root/.ivy2/cache\nThe jars for the packages stored in: /root/.ivy2/jars\norg.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-e696dc1c-05f5-443d-8260-4477f1e784d9;1.0\n\tconfs: [default]\n\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 in central\n\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 in central\n\tfound org.apache.kafka#kafka-clients;2.6.0 in central\n\tfound com.github.luben#zstd-jni;1.4.8-1 in central\n\tfound org.lz4#lz4-java;1.7.1 in central\n\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n\tfound org.slf4j#slf4j-api;1.7.30 in central\n\tfound org.spark-project.spark#unused;1.0.0 in central\n\tfound org.apache.commons#commons-pool2;2.6.2 in central\n:: resolution report :: resolve 532ms :: artifacts dl 10ms\n\t:: modules in use:\n\tcom.github.luben#zstd-jni;1.4.8-1 from central in [default]\n\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n\torg.apache.kafka#kafka-clients;2.6.0 from central in [default]\n\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 from central in [default]\n\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 from central in [default]\n\torg.lz4#lz4-java;1.7.1 from central in [default]\n\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n\torg.spark-project.spark#unused;1.0.0 from central in [default]\n\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n\t---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent-e696dc1c-05f5-443d-8260-4477f1e784d9\n\tconfs: [default]\n\t0 artifacts copied, 9 already retrieved (0kB/12ms)\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n21/11/10 21:06:32 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.2.jar added multiple times to distributed cache.\n21/11/10 21:06:32 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.2.jar added multiple times to distributed cache.\n21/11/10 21:06:32 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar added multiple times to distributed cache.\n21/11/10 21:06:32 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar added multiple times to distributed cache.\n21/11/10 21:06:32 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar added multiple times to distributed cache.\n21/11/10 21:06:32 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar added multiple times to distributed cache.\n21/11/10 21:06:32 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar added multiple times to distributed cache.\n21/11/10 21:06:32 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar added multiple times to distributed cache.\n21/11/10 21:06:32 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar added multiple times to distributed cache.\n"}], "source": "from pyspark.sql import SparkSession\n\n# first we enrich our Spark session with some knowledge about Kafka\nspark = (\n    SparkSession.builder\n    # integration with Kafka is done as recommended by the official doc:\n    # http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\")\n    # setting a checkpoint directory is mandatory\n    # without it streaming jobs simply won't work\n    .config(\"spark.sql.streaming.checkpointLocation\", \"checkpoints\")\n    .getOrCreate()\n)"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": "input_topic = (\n    spark.readStream.format(\"kafka\")\n    .option(\"subscribe\", \"input_topic\")\n    # this should come from the cluster configuration page\n    .option(\n        \"kafka.bootstrap.servers\",\n        \"pkc-4r297.europe-west1.gcp.confluent.cloud:9092\"\n    )\n    # these two lines are similar to configuring `kafka-python`\n    .option(\"kafka.security.protocol\", \"SASL_SSL\")\n    .option(\"kafka.sasl.mechanism\",\"PLAIN\")\n    # this mysterious line must include API key and secret from the cluster\n    # see more in the documentation https://kafka.apache.org/documentation/#security_jaas_broker\n    .option(\n        \"kafka.sasl.jaas.config\",\n        \"\"\"org.apache.kafka.common.security.plain.PlainLoginModule required\n        username=\"4KLAOHXKUM6GFMLJ\"\n        password=\"tdpFCiSB+pMISzXXsLnkBeYZAKc9+lb4rIRWcuv7simAlUWUbqRi8KxUt21w5+XY\";\n        \"\"\"\n    )\n    .load()\n)"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": "from pyspark.sql.functions import col\n\n# in Spark we use nearly identical DataFrame API\n# both for files on disks and for streams\ntransformations = (\n    input_topic\n    .select(\n        (\n            # by default, the streaming DataFrame has `value` column\n            # it contain bytes, so first it should be decoded as a string\n            # then we transform it to float and double\n            col(\"value\").astype(\"string\").astype(\"float\") * 2\n        # to save data, we encode it to string (Spark will make them bytes for us)\n        # also, only `value` and other Kafka-related columns are written back\n        ).astype(\"string\").alias(\"value\")\n    )\n)"}, {"cell_type": "code", "execution_count": 6, "metadata": {"tags": []}, "outputs": [], "source": "# write stream back to Kafka\noutput_topic = (\n    transformations\n    .writeStream\n    .format(\"kafka\")\n    .option(\"topic\", \"output_topic\")\n    # these line are the same as in the input topic\n    .option(\n        \"kafka.bootstrap.servers\",\n        \"pkc-4r297.europe-west1.gcp.confluent.cloud:9092\"\n    )\n    .option(\"kafka.security.protocol\", \"SASL_SSL\")\n    .option(\"kafka.sasl.mechanism\",\"PLAIN\")\n    .option(\n        \"kafka.sasl.jaas.config\",\n        \"\"\"org.apache.kafka.common.security.plain.PlainLoginModule required\n        username=\"4KLAOHXKUM6GFMLJ\"\n        password=\"tdpFCiSB+pMISzXXsLnkBeYZAKc9+lb4rIRWcuv7simAlUWUbqRi8KxUt21w5+XY\";\n        \"\"\"\n    )\n)"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "21/11/10 21:06:49 WARN org.apache.spark.sql.streaming.StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n[Stage 97:=============================================>        (170 + 2) / 200]\r"}], "source": "# the most important part! It's an actions. Everything else were transformations\njob = output_topic.start()"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "21/11/10 21:08:53 ERROR org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@503a9df2 is aborting.\n21/11/10 21:08:53 ERROR org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@503a9df2 aborted.\n21/11/10 21:08:53 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 158.0 in stage 97.0 (TID 9827) (cluster-b2c2-w-0.europe-west1-c.c.aiqdsc22.internal executor 1): TaskKilled (Stage cancelled)\n"}], "source": "# job runs asynchronously\n# simply changing its definition and starting again won't kill it\n# stop your jobs gracefully\njob.stop()"}, {"cell_type": "markdown", "metadata": {}, "source": "# Do it Yourself\n\n* compute average value and the number of values\n* inside a 10 seconds window\n* output the window description and the computation results to Kafka"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.12"}, "name": "spark-streaming.ipynb"}, "nbformat": 4, "nbformat_minor": 4}