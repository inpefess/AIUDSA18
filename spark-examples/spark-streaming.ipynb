{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\n# first we enrich our Spark session with some knowledge about Kafka\nspark = (\n    SparkSession.builder\n    # integration with Kafka is done as recommended by the official doc:\n    # http://spark.apache.org/docs/3.2.1/streaming-kafka-0-10-integration.html\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1\")\n    # setting a checkpoint directory is mandatory\n    # without it streaming jobs simply won't work\n    .config(\"spark.sql.streaming.checkpointLocation\", \"checkpoints\")\n    .getOrCreate()\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"997f59a1-fb27-408d-834d-41df690d9792","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["input_topic = (\n    spark.readStream.format(\"kafka\")\n    .option(\"subscribe\", \"input_topic\")\n    .option(\n        \"kafka.bootstrap.servers\",\n        \"put the Kafka cluster address here\"\n    )\n    # these two lines are similar to configuring `kafka-python`\n    .option(\"kafka.security.protocol\", \"SASL_SSL\")\n    .option(\"kafka.sasl.mechanism\",\"PLAIN\")\n    # this mysterious line must include API key and secret from the cluster\n    # see more in the documentation https://kafka.apache.org/documentation/#security_jaas_broker\n    .option(\n        \"kafka.sasl.jaas.config\",\n        # remove kafkashaded prefix when not on Databricks\n        \"\"\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required\n        username=\"use API key as username\"\n        password=\"use API secret as username\";\n        \"\"\"\n    )\n    .load()\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0557df25-9690-4e44-befc-7537b11d4c78","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Spark has a feature called Structured Streaming\n# the idea is that streams are DataFrames\nprint(type(input_topic))\ninput_topic"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"90758579-1ed0-42c6-922e-9c8dbc4baeb8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<class 'pyspark.sql.dataframe.DataFrame'>\nOut[3]: DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["<class 'pyspark.sql.dataframe.DataFrame'>\nOut[3]: DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]"]}}],"execution_count":0},{"cell_type":"code","source":["# unfortunately, it doesn't work with pandas API, only with Spark DataFrames\ninput_topic.to_pandas_on_spark()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"67a8d591-4a98-4bfa-a19a-e91a292b981a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-849733004669718>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# unfortunately, it doesn't work with pandas API, only with Spark DataFrames\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0minput_topic\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_pandas_on_spark\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mto_pandas_on_spark\u001B[0;34m(self, index_col)\u001B[0m\n\u001B[1;32m   2955\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2956\u001B[0m         \u001B[0mindex_spark_columns\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex_names\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_get_index_map\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex_col\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2957\u001B[0;31m         internal = InternalFrame(\n\u001B[0m\u001B[1;32m   2958\u001B[0m             \u001B[0mspark_frame\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex_spark_columns\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mindex_spark_columns\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex_names\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mindex_names\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2959\u001B[0m         )\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/internal.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, spark_frame, index_spark_columns, index_names, index_fields, column_labels, data_spark_columns, data_fields, column_label_names)\u001B[0m\n\u001B[1;32m    616\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    617\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mspark_frame\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mSparkDataFrame\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 618\u001B[0;31m         \u001B[0;32massert\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mspark_frame\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0misStreaming\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"pandas-on-Spark does not support Structured Streaming.\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    619\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    620\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mindex_spark_columns\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAssertionError\u001B[0m: pandas-on-Spark does not support Structured Streaming.","errorSummary":"<span class='ansi-red-fg'>AssertionError</span>: pandas-on-Spark does not support Structured Streaming.","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-849733004669718>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# unfortunately, it doesn't work with pandas API, only with Spark DataFrames\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0minput_topic\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_pandas_on_spark\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mto_pandas_on_spark\u001B[0;34m(self, index_col)\u001B[0m\n\u001B[1;32m   2955\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2956\u001B[0m         \u001B[0mindex_spark_columns\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex_names\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_get_index_map\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex_col\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2957\u001B[0;31m         internal = InternalFrame(\n\u001B[0m\u001B[1;32m   2958\u001B[0m             \u001B[0mspark_frame\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex_spark_columns\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mindex_spark_columns\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex_names\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mindex_names\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2959\u001B[0m         )\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/internal.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, spark_frame, index_spark_columns, index_names, index_fields, column_labels, data_spark_columns, data_fields, column_label_names)\u001B[0m\n\u001B[1;32m    616\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    617\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mspark_frame\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mSparkDataFrame\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 618\u001B[0;31m         \u001B[0;32massert\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mspark_frame\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0misStreaming\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"pandas-on-Spark does not support Structured Streaming.\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    619\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    620\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mindex_spark_columns\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAssertionError\u001B[0m: pandas-on-Spark does not support Structured Streaming."]}}],"execution_count":0},{"cell_type":"markdown","source":["The documentation mentions a [workaround](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/faq.html#does-pandas-api-on-spark-support-structured-streaming) to use pandas-on-Spark with Structured Streaming.\n\nUnfortunately, it means you write your stream to nowhere and do something with batches of this stream as a side effect.\n\nThe drawbacks of it are:\n* you have to care about writing you stream somewhere yourself\n* you always work with data in mini-batches, no continuous streams\n* you can't use advanced streaming features like [watermarks](http://spark.apache.org/docs/3.2.1/structured-streaming-programming-guide.html#handling-late-data-and-watermarking)\n\nSo, we are not going for it."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eca2b664-cc48-4e5a-a599-61aec9f041ff","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# in Spark, we use identical DataFrame API both for files on disks and for streams\ntransformed_dataframe = (\n    input_topic\n    .select(\n        (\n            # by default, the streaming DataFrame has `value` column\n            # it contain bytes, so first it should be decoded as a string\n            # then we transform it to float\n            10 * input_topic[\"value\"].astype(\"string\").astype(\"float\")\n        # to save data, we encode it to string (Spark will make them bytes for us)\n        # Kafka expects only key and value columns to be in the data at most\n        ).astype(\"string\").alias(\"value\")\n    )\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"56817d9c-e770-402a-958a-1f608b80adaa","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# we don't need to add other columns like timestamp or partition\n# Kafka sink will add them automatically\ntransformed_dataframe"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"557ab404-2a1a-44e5-8c7a-a5ce0a8e12e9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[6]: DataFrame[value: string]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[6]: DataFrame[value: string]"]}}],"execution_count":0},{"cell_type":"code","source":["# write stream back to Kafka\noutput_topic = (\n    transformed_dataframe\n    .writeStream\n    .format(\"kafka\")\n    .option(\"topic\", \"output_topic\")\n    # these line are the same as in the input topic\n    .option(\n        \"kafka.bootstrap.servers\",\n        \"put the Kafka cluster address here\"\n    )\n    .option(\"kafka.security.protocol\", \"SASL_SSL\")\n    .option(\"kafka.sasl.mechanism\",\"PLAIN\")\n    .option(\n        \"kafka.sasl.jaas.config\",\n        \"\"\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required\n        username=\"use API key as username\"\n        password=\"use API secret as username\";\n        \"\"\"\n    )\n)"],"metadata":{"tags":[],"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5911b449-9082-4073-8de7-ae0e82c7d326","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# the most important part! It's an action. Everything else were transformations\njob = output_topic.start()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9c7ee3ae-2e88-48b4-92ea-8698d162c9c8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# job runs asynchronously\n# simply changing its definition and starting again won't kill it\n# stop your jobs gracefully\njob.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c0175e5b-76a3-46de-98e5-c85c6edbbe46","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Do it Yourself\n\n* compute average value and the number of values\n* inside a 10 seconds window\n* output the window description and the computation results to Kafka"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a62f6215-7365-47c4-8c8e-f37741a453e2","inputWidgets":{},"title":""}}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.8.12","nbconvert_exporter":"python","file_extension":".py"},"name":"spark-streaming.ipynb","application/vnd.databricks.v1+notebook":{"notebookName":"spark-streaming","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":849733004669702}},"nbformat":4,"nbformat_minor":0}
