{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Session\n",
    "\n",
    "Spark can work with data located on HDFS or a non-distributed filesystem. It can also use YARN from Hadoop, or [Mesos](https://mesos.apache.org/), or a resource manager of its own.\n",
    "\n",
    "All distributed operations with Spark are done using so-called Spark Session. Usually one is already created by your cluster's administrator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-edbb-m.europe-west6-b.c.tactical-racer-294709.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f28578f4290>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data\n",
    "\n",
    "Spark can consume data in a variety of formats, e.g. in JSON. We use the [YELP Dataset](https://www.yelp.com/dataset) for this example. It's easily obtainable and free to use in education and research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_on_hdfs = \"/user/borisshminke/data/yelp_academic_dataset_review.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.41 ms, sys: 4.32 ms, total: 8.73 ms\n",
      "Wall time: 14.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8021122"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark.read.text(reviews_on_hdfs).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code simply reads a JSON file as a text, line by line, and counts the number of lines. Let's compare the speed with `wc` tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8021122 /home/borisshminke/Downloads/yelp_academic_dataset_review.json\n",
      "CPU times: user 44.9 ms, sys: 13.3 ms, total: 58.2 ms\n",
      "Wall time: 1.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!wc -l /home/borisshminke/Downloads/yelp_academic_dataset_review.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although `wc` is implemented in C and is more efficient in general than JVM code behind Spark, it uses only one CPU, and sometimes may work slower than it's distributed counterpart from Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing JSON in Spark is really simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|         business_id|cool|               date|funny|           review_id|stars|                text|useful|             user_id|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|-MhfebM0QIsKt87iD...|   0|2015-04-15 05:21:16|    0|xQY8N_XvtGbearJ5X...|  2.0|As someone who ha...|     5|OwjRMXRC0KyPrIlcj...|\n",
      "|lbrU8StCq3yDfr-QM...|   0|2013-12-07 03:16:52|    1|UmFMZ8PyXZTY2Qcwz...|  1.0|I am actually hor...|     1|nIJD_7ZXHq-FX8byP...|\n",
      "|HQl28KMwrEKHqhFrr...|   0|2015-12-05 03:18:11|    0|LG2ZaYiOgpr2DK_90...|  5.0|I love Deagan's. ...|     1|V34qejxNsCbcgD8C0...|\n",
      "|5JxlZaqCnk1MnbgRi...|   0|2011-05-27 05:30:52|    0|i6g_oA9Yf9Y31qt0w...|  1.0|Dismal, lukewarm,...|     0|ofKDkJKXSKZXu5xJN...|\n",
      "|IS4cv902ykd8wj1TR...|   0|2017-01-14 21:56:57|    0|6TdNDKywdbjoTkize...|  4.0|Oh happy day, fin...|     0|UgMW8bLE0QMJDCkQ1...|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews = spark.read.json(reviews_on_hdfs)\n",
    "reviews.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "|                day|total_cool|\n",
      "+-------------------+----------+\n",
      "|2019-01-06 00:00:00|      3133|\n",
      "|2019-01-02 00:00:00|      3017|\n",
      "|2018-04-09 00:00:00|      2897|\n",
      "|2018-03-13 00:00:00|      2810|\n",
      "|2018-04-23 00:00:00|      2709|\n",
      "+-------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spark can be used similarly to Pandas\n",
    "from pyspark.sql import functions as sf\n",
    "\n",
    "(\n",
    "    reviews\n",
    "    .groupby(sf.date_trunc(\"day\", \"date\").alias(\"day\"))\n",
    "    .agg(sf.sum(\"cool\").alias(\"total_cool\"))\n",
    "    .sort(sf.desc(\"total_cool\"))\n",
    "    .show(n=5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "|                day|total_cool|\n",
      "+-------------------+----------+\n",
      "|2019-01-06 00:00:00|      3133|\n",
      "|2019-01-02 00:00:00|      3017|\n",
      "|2018-04-09 00:00:00|      2897|\n",
      "|2018-03-13 00:00:00|      2810|\n",
      "|2018-04-23 00:00:00|      2709|\n",
      "+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# or you can use Spark as an SQL engine\n",
    "\n",
    "reviews.createOrReplaceTempView(\"reviews\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    date_trunc('day', date) AS day,\n",
    "    SUM(cool) AS total_cool\n",
    "FROM reviews\n",
    "GROUP BY\n",
    "    day\n",
    "ORDER BY\n",
    "    total_cool DESC\n",
    "LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do It Yourself\n",
    "\n",
    "[Spark Manual](http://spark.apache.org/docs/2.4.3/api/python/index.html) is your best friend!\n",
    "\n",
    "* count number of users, buisenesses\n",
    "* count average number of reviews and stars per business and per user\n",
    "* find histograms for distributions of cool, funny, and useful columns\n",
    "* find ten most frequent words from the reviews\n",
    "* save results to disk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
